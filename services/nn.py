# -*- coding: utf-8 -*-
"""grain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d8n0bNqcOmIGRmEq3nxnBh7R0LPwe94x

**Importing the necessary libraries**

1. `pandas as pd` Pandas is a python library used for data manipulations
2. `sklearn.*` Scikit-Learn is a Machine Learning framework
3. `category_encoders` Category Encoders is a data preprocessing library specialized in Categorical Encoding
"""

import pandas as pd
from sklearn.impute import KNNImputer
from category_encoders import CountEncoder
from sklearn.cluster import KMeans
from math import pi
import cv2
from skimage import io
import numpy as np

"""**Data preprocessing**


1. Dropping columns with 70 % or more null/NaN values and columns with low correlation standards like brand of the equipment used.
2. Encoding categorical data with a `CountEncoder()` for encoding the frequency.
3. Imputing missing data rows with a `KNNImputer()` with `k = 3`.
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import cv2
import numpy as np
from matplotlib import pyplot as plt
from scipy import ndimage
from skimage import measure, color, io


def get_grain_size(filepath, width, iter=0, image=False):
    """
  GET average grain size from image path
  """
    img = cv2.imread(filepath, 0)
    h, w, = img.shape
    avg_size = (h + w) / 2
    # print(avg_size)
    pixels_to_um = width / avg_size  # (avg_size px = width um) -> 1 px = (width/avg_size) um
    ret, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    kernel = np.ones((3, 3), np.uint8)
    eroded = cv2.erode(thresh, kernel, iterations=iter)
    dilated = cv2.dilate(eroded, kernel, iterations=iter)

    mask = dilated == 255  # Sets TRUE for all 255 valued pixels and FALSE for 0

    s = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]
    labeled_mask, num_labels = ndimage.label(mask, structure=s)

    img2 = color.label2rgb(labeled_mask, bg_label=0)

    if image:
        io.imshow(img2)

    clusters = measure.regionprops_table(labeled_mask, img, properties=['equivalent_diameter', 'area', 'perimeter'])
    return [clusters, (clusters['equivalent_diameter'] * pixels_to_um).mean()]

def get_closest_color_name(target_color, color_palette):
    color_names = ['Gray', 'Yellow', 'Brown', 'Red', 'White', 'Pink', 'Black', 'Beige', 'Green', 'Blue']

    # Convert target color to numpy array in BGR format
    target_bgr = np.array(target_color, dtype=np.uint8)

    # Convert color palette to numpy array in BGR format
    color_palette = np.array(color_palette, dtype=np.uint8)

    # Calculate Euclidean distances
    distances = np.linalg.norm(color_palette - target_bgr, axis=1)

    # Find index of the closest color
    closest_color_index = np.argmin(distances)

    # Return the name of the closest color
    closest_color_name = color_names[closest_color_index]
    return closest_color_name

color_palette = [
    (128, 128, 128),  # Gray
    (0, 255, 255),  # Yellow
    (102, 176, 255),  # Brown
    (0, 0, 255),  # Red
    (255, 255, 255),  # White
    (193, 182, 255),  # Pink
    (0, 0, 0),  # Black
    (220, 245, 245),  # Beige
    (0, 255, 0),  # Green
    (255, 0, 0),  # Blue
]

def classify_stone(grain_size, color_g):
    results = []
    if (grain_size <= 4) and (color_g == 'Gray'):
        results.append("Shale")
    if (57 < grain_size <= 2000) and (color_g in ['Gray', 'Yellow', 'Brown', 'Red']):
        results.append("Sandstone")
    if (grain_size >= 1) and (color_g in ['Gray', 'Yellow', 'White']):
        results.append("Limestone")
    if (57 <= grain_size <= 2000) and (color_g in ['Pink', 'Gray', 'Black', 'White', 'Brown', 'Red', 'Beige']):
        results.append("Granite")
    if (45 <= grain_size <= 250) and (color_g in ['White', 'Gray', 'Pink', 'Yellow', 'Blue', 'Green']):
        results.append("Quartzite")
    if (7 <= grain_size <= 2000) and (color_g in ['Black', 'Green', 'Red']):
        results.append("Tuff")

    if len(results) > 1:
        result_string = " or ".join(results)
        print(f'Stone type: {result_string}')
        return result_string
    elif results:
        print(f'Stone type: {results[0]}')
        return results[0]
    else:
        print("Unable to classify the stone type.")
        return -1

def get_lithotype(macrofilename, microfilename):

    KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,
           n_clusters=1, n_init=10,
           random_state=None, tol=0.0001, verbose=0)
    img = cv2.imread(macrofilename, cv2.IMREAD_UNCHANGED)

    data = np.reshape(img, (-1, 3))
    print(data.shape)
    data = np.float32(data)

    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
    flags = cv2.KMEANS_RANDOM_CENTERS
    compactness, labels, centers = cv2.kmeans(data, 1, None, criteria, 10, flags)

    target = tuple(centers[0].astype(int))
    color_grain = get_closest_color_name(target, color_palette)

    width = 1300
    grain_size = get_grain_size(microfilename, width, 0, True)[1]
    return classify_stone(grain_size, color_grain)

# color_palette = [
#     (128, 128, 128),  # Gray
#     (255, 255, 0),  # Yellow
#     (165, 42, 42),  # Brown
#     (255, 0, 0),  # Red
#     (255, 255, 255),  # White
#     (255, 182, 193),  # Pink
#     (0, 0, 0),  # Black
#     (245, 245, 220),  # Beige
#     (0, 128, 0),  # Green
#     (0, 0, 255),  # Blue
# ]

# xdf = pd.read_csv('imputed_x.csv').drop(columns=['Unnamed: 0', 'CALI', 'RSHA', 'RMED', 'PEF', 'DTC', 'SP'])
# ydf = pd.read_csv('train_data.csv')[
#     ['DEPTH_MD', 'FORCE_2020_LITHOFACIES_LITHOLOGY', 'FORCE_2020_LITHOFACIES_CONFIDENCE']]
# df = pd.merge(xdf, ydf, on='DEPTH_MD')

lithology_map = {
    30000: 'Sandstone',
    65030: 'Sandstone/Shale',
    65000: 'Shale',
    80000: 'Marl',
    74000: 'Dolomite',
    70000: 'Limestone',
    70032: 'Chalk',
    88000: 'Halite',
    86000: 'Anhydrite',
    99000: 'Tuff',
    90000: 'Coal',
    93000: 'Basement'
}

# df = df.replace({'FORCE_2020_LITHOFACIES_LITHOLOGY': lithology_map})

sorting_map = {
    -1: "N/A",
    0: "Very Well Sorted",
    1: "Well Sorted",
    2: "Moderately Sorted",
    3: "Poorly Sorted",
    4: "Very Poorly Sorted",
    5: "Extremely Poorly Sorted",
}

def get_sorting(cluster):
    phi84 = np.percentile(cluster['equivalent_diameter'], 84)
    phi16 = np.percentile(cluster['equivalent_diameter'], 16)
    phi95 = np.percentile(cluster['equivalent_diameter'], 95)
    phi5 = np.percentile(cluster['equivalent_diameter'], 5)

    igsd = round(((phi84 - phi16) / 4.) + ((phi95 - phi5) / 6.6), 3)
    sorting = -1
    if igsd <= 0.35:
        sorting = 0
    elif (igsd > 0.35) and (igsd <= 0.5):
        sorting = 1
    elif (igsd > 0.5) and (igsd <= 1):
        sorting = 2
    elif (igsd > 1) and (igsd <= 2):
        sorting = 3
    elif (igsd > 2) and (igsd <= 4):
        sorting = 4
    else:
        sorting = 5
    print('Inclusive Graphic Standard Deviation, value: {}, sorting: {}'.format(igsd, sorting_map[sorting]))
    return igsd, sorting_map[sorting]

def get_roundness(cluster):
    circulars_count = 0
    for area, perimeter in zip(cluster['area'], cluster['perimeter']):
        if perimeter == 0:
            continue
        circularity = 4 * pi * (area / (perimeter * perimeter))
        if 0.7 < circularity < 1.2:
            circulars_count += 1
    return circulars_count / len(cluster['area'])

def draw_grid(img, grid_shape, color=(0, 255, 0), thickness=1):
    h, w = img.shape
    rows, cols = grid_shape
    dy, dx = h / rows, w / cols

    # draw vertical lines
    for x in np.linspace(start=dx, stop=w - dx, num=cols - 1):
        x = int(round(x))
        cv2.line(img, (x, 0), (x, h), color=color, thickness=thickness)

    # draw horizontal lines
    for y in np.linspace(start=dy, stop=h - dy, num=rows - 1):
        y = int(round(y))
        cv2.line(img, (0, y), (w, y), color=color, thickness=thickness)

    return img


def threshold(image, min_limit=None, max_limit=255, clip=0):
    if min_limit is None:
        min_limit = int(np.mean(image) + clip)

    _, image = cv2.threshold(image,
                             min_limit,
                             max_limit,
                             cv2.THRESH_BINARY)
    return image


def get_approx_fractures(filepath):
    img = cv2.imread(filepath, 0)
    height, width = img.shape
    subsets = {}
    window = 48
    for i in range(0, height, window):
        for j in range(0, width, window):
            subset = img[j:j + window, i:i + window]
            subsets['{}.{}'.format(j, i)] = subset

    train_img = cv2.erode(threshold(img), (3, 3), iterations=7)
    blurred_img = cv2.GaussianBlur(train_img, (7, 7), 5)

    contours, _ = cv2.findContours(blurred_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contour_image = img.copy()
    cv2.drawContours(contour_image, contours, -1, (0, 255, 0), 2)
    return len(contours), contour_image, contours
